{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "---\n",
    "\n",
    "Das ist eine Zusammenfassung bzw. ein Cheatsheet zum Thema **Natural Language Processing**.<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "## Table of Contents\n",
    "<br>\n",
    "\n",
    "### 1. Basics\n",
    "* Language models\n",
    "<br>\n",
    "* Word Vectors\n",
    "<br>\n",
    "* Co-Occurence matrix\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Einführung in die Language Models\n",
    "<br>\n",
    "\n",
    "Language Models können verschiedene NLP Probleme wie Spell Checking, Machine Translation oder auch Speech Recognition lösen. Die Grundidee ist im Endeffekt, für einen bestimmten Satz oder für bestimmte Wörter Wahrscheinlichkeiten auszurechnen.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Spell Checking\n",
    "Beim Spell Checking möchte man z.B. für einen Satz \"The office is about fifteen minuets from my house\" erreichen, dass **P(\"about fifteen minutes from\") > P(\"about fifteen minuets from\")** gilt.\n",
    "\n",
    "#### Machine Translation\n",
    "Bei der Machine Translation möchte man mit den Wahrscheinlichkeiten gute von schlechten Übersetzungen unterscheiden, z.B. \n",
    "**P(\"High winds tonite\") > P(\"Large winds tonite\")**, da erstere eine bessere Übersetzung darstellt.\n",
    "\n",
    "<br>\n",
    "\n",
    "Was wir im Endeffekt also möchten ist ein Model, dass für einen String W = w1....wn die Wahrscheinlichkeit\n",
    "<br>\n",
    "**<p style=\"text-align: center;\">P(W) = P(w1,w2,....,wn)</p>** oder \n",
    "<br>\n",
    "**<p style=\"text-align: center;\">P(wn | w1,w2,...wn-1)</p>**\n",
    "<br>\n",
    "berechnen kann.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Wie werden die Wahrscheinlichkeiten berechnet?\n",
    "Die Wahrscheinlichkeiten werden folgendermaßen berechnet:\n",
    "\n",
    "![Markov Assumption](img/markov-assumption.png)\n",
    "\n",
    "Dabei geht es darum, nicht für jedes Wort den gesamten Kontext zu betrachten (zu kompliziert, zu viele Möglichkeiten), sondern man betrachtet für jedes Wort nur k Kontextwörter.\n",
    "<br>\n",
    "Für **k=0** nennt man solche Models **Unigram Models**, diese betrachten keinerlei Kontext, d.h. die Wahrscheinlichkeit eines Satzes ist einfach das Produkt der Wahrscheinlichkeiten aller Einzelwörter. Für **k=1** spricht man von **Bigram Models**, die das jeweils hintere Wort betrachten etc. \n",
    "\n",
    "<br>\n",
    "\n",
    "Die Einzelwahrscheinlichkeiten werden über die **Counts** geschätzt. Für ein Bigram Model ginge das folgendermaßen:\n",
    "\n",
    "<br>\n",
    "\n",
    "![Count Probability](img/count-probability.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Mit c als \"Count-Funktion\" berechnet das also, wie oft **wi-1** und **wi** gemeinsam vorkommen geteilt durch die Anzahl der Vorkommen von **wi-1**. Dies wird nun für ein ausreichend großes Dataset für jedes Wort im Trainingsset berechnet. Das Ergebnis ist ein Language Model, dass dann für einen konkretes Problem verwendet werden kann.\n",
    "<br>\n",
    "In der Praxis werden diese Wahrscheinlichkeiten aus Effizienzgründen aber nicht miteinander multipliziert, sondern man logarithmiert und addiert sie in der Form **P1 x P2 = log(P1) + log(P2)**.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Word Vectors\n",
    "<br>\n",
    "\n",
    "In der Computerlinguistik werden Wörter in der Regel als Vektoren dargestellt, um sie maschinell verarbeiten zu können. \n",
    "Dabei können Wörter auf verschiedene Arten als Vektoren dargestellt werden, wobei die konkrete Herangehensweise ziemlich vom Anwendungsfall abhängt. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### One-Hot-Vectors\n",
    "Bei den sogenannten **One-Hot-Vectors** handelt es sich um Vektoren, die nur aus den Werten 0 und 1 bestehen.\n",
    "Diese finden meist Anwendung, wenn man gegeben einem **Vokabular V** bestehend aus n verschiedenen Wörtern und \n",
    "einer **Dokumentensammlung D** für jedes Dokument d speichern möchte, ob das jeweilige Wort in Dokument d vorkommt oder nicht.\n",
    "<br>\n",
    "Dabei bestehen die One-Hot-Vectors aus insgesamt n Komponenten (Größe des Vokabulars), wobei der Wert der jeweiligen Vektorkomponente\n",
    "1 ist, wenn das Wort in Dokument d vorkommt und 0 wenn nicht.\n",
    "<br>\n",
    "<br>\n",
    "\"Stapelt\" man die One-Hot-Vectors aller Dokumente, ergibt sich eine Matrix mit N Zeilen und K Spalten, wobei N die Anzahl der Dokumente\n",
    "und K die Vokabulargröße ist. Aus dieser kann man dann direkt ablesen, welche Wörter in welchen Dokumenten vorkommen.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Count-based Vectors\n",
    "Im Gegensatz zu den One-Hot-Vectors können hier die Vektorkomponenten verschiedene Werte >= 0 annehmen. Die Herangehensweise ist die selbe,\n",
    "jedoch wird für jedes Wort nun nicht gespeichert, ob es vorkommt oder nicht, sondern wie oft. Somit lässt sich aus der Designmatrix später ablesen,\n",
    "welche Wörter **wie oft** in welchen Dokumenten vorkommen.++\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Context-based Word Vectors\n",
    "Für bestimmte NLP Probleme lässt sich das Konzept auch insofern erweitern, dass man für jedes Wort dessen Kontext speichert.\n",
    "Dabei wird eine bestimmte Fensterlänge n festgelegt, und für jedes Wort w(i) im Vokabular wird gespeichert, welches Wort w(j) in der \n",
    "n-Umgebung von Wort w(i) vorkommt. Dies ist wichtig, da sich herausgestellt hat, dass die Bedeutung eines Wortes maßgeblich dadurch\n",
    "bestimmt wird, welche Wörter in seiner Umgebung / in seinem Kontext vorkommen. \n",
    "<br>\n",
    "Dies lässt sich dann in einer Co-Occurence-Matrix darstellen, \n",
    "die die Form V x V besitzt (V = Vokabulargröße).\n",
    "\n",
    "<br>\n",
    "\n",
    "## Co-Occurence-Matrix\n",
    "Eine Co-Occurence_Matrix für das Vokabular V = { all, that, glitters, is, not, gold, well, ends }, der Fenstergröße n = 1 und den zwei Dokumenten:\n",
    "<br>\n",
    "<br>\n",
    "Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "Document 2: \"all is well that ends well\"\n",
    "\n",
    "würde somit folgendermaßen aussehen:\n",
    "<br>\n",
    "\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
