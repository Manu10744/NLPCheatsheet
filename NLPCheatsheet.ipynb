{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "\n",
    "import nltk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "# ==================== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "---\n",
    "\n",
    "Das ist eine Zusammenfassung bzw. ein Cheatsheet zum Thema **Natural Language Processing**.<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "## Table of Contents\n",
    "<br>\n",
    "\n",
    "### 1. Basics\n",
    "* Text Processing\n",
    "<br>\n",
    "* Language models\n",
    "<br>\n",
    "* Word Vectors\n",
    "<br>\n",
    "* Co-Occurence matrix\n",
    "<br>\n",
    "* Wortähnlichkeit\n",
    "<br>\n",
    "\n",
    "### 2. Spelling Correction\n",
    "* Noisy Channel Model\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Basics\n",
    "\n",
    "## Text Processing\n",
    "Bevor man mit natürlicher Sprache arbeiten kann, muss man den Text oft erst einmal verarbeiten. Die wichtigsten Schritte:\n",
    "- **Tokenization:** Zerlegen von Sätzen in einzelne Wörter.\n",
    "- **Normalization:** Enthält mehrere Schritte, z.\n",
    "- **Stopword removal:** Entfernen von für eine Sprache typisch häufig vorkommenden und daher nicht wertvollen Wörtern (z.B. 'the').\n",
    "- **Filtering:** Entfernen von ungewünschten Symbolen, wie z.B. Sonderzeichen. (Meist durch RegEx)\n",
    "- **Lemmatizing:** Zurückführen aller Wörter auf ihren Wortstamm, d.h. eliminieren jeglicher grammatikalischer Veränderungen.\n",
    "- **Stemming:** Auch hier Zurückführen aller Wörter auf eine Grundform, aber aggressiver als beim Lemmatizing. Ergebnis ist oft nicht der vollständige, lexikalische Wortstamm.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      " [['Heute', 'ist', 'das', 'Wetter', 'wieder', 'schlecht', ',', 'denn', 'für', 'den', 'ganzen', 'Tag', 'wurde', 'Regen', 'angesagt', '.'], ['Präsident', 'Trump', 'hat', 'auch', 'heute', 'wieder', 'unfassbaren', 'Mist', 'auf', 'Twitter', 'geschrieben', '.'], ['Die', 'Coronavirus-Pandemie', 'hält', 'die', 'Welt', 'weiterhin', 'in', 'Atem', ':', 'Mehr', 'als', '5', 'Millionen', 'Menschen', 'haben', 'sich', 'weltweit', 'mit', 'dem', 'neuartigen', 'Covid-19', 'infiziert', '–', '177.183', 'davon', 'bisher', 'in', 'Deutschland', '.']] \n",
      "\n",
      "Normalized:\n",
      " [['<START>', 'heute', 'ist', 'das', 'wetter', 'wieder', 'schlecht', ',', 'denn', 'für', 'den', 'ganzen', 'tag', 'wurde', 'regen', 'angesagt', '.', '<END>'], ['<START>', 'präsident', 'trump', 'hat', 'auch', 'heute', 'wieder', 'unfassbaren', 'mist', 'auf', 'twitter', 'geschrieben', '.', '<END>'], ['<START>', 'die', 'coronavirus-pandemie', 'hält', 'die', 'welt', 'weiterhin', 'in', 'atem', ':', 'mehr', 'als', '5', 'millionen', 'menschen', 'haben', 'sich', 'weltweit', 'mit', 'dem', 'neuartigen', 'covid-19', 'infiziert', '–', '177.183', 'davon', 'bisher', 'in', 'deutschland', '.', '<END>']] \n",
      "\n",
      "Length of original: 63\n",
      "Length after stopword removal: 44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "test_corpus = [\n",
    "    \"Heute ist das Wetter wieder schlecht, denn für den ganzen Tag wurde Regen angesagt.\",\n",
    "    \"Präsident Trump hat auch heute wieder unfassbaren Mist auf Twitter geschrieben.\",\n",
    "    \"Die Coronavirus-Pandemie hält die Welt weiterhin in Atem: Mehr als 5 Millionen Menschen haben sich weltweit mit dem neuartigen Covid-19 infiziert – 177.183 davon bisher in Deutschland.\",\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_sents = [nltk.tokenize.word_tokenize(sentence) for sentence in test_corpus]\n",
    "print(\"Tokenized:\\n {} \\n\".format(tokenized_sents))\n",
    "\n",
    "# Normalization (doesn't need to be done seperately)\n",
    "for sent in tokenized_sents:\n",
    "    for idx, word in enumerate(sent):\n",
    "        sent[idx] = word.lower()\n",
    "    sent.insert(0, START_TOKEN)\n",
    "    sent.insert(len(sent), END_TOKEN)\n",
    "    \n",
    "print(\"Normalized:\\n {} \\n\".format(tokenized_sents))\n",
    "\n",
    "# Join the lists so its easier to process the corpus\n",
    "tokenized_sents = [token for sentence in tokenized_sents for token in sentence]\n",
    "\n",
    "# Stopword removal\n",
    "german_stopwords = nltk.corpus.stopwords.words('german')\n",
    "sw_removed = [token for token in tokenized_sents if token not in german_stopwords]\n",
    "\n",
    "print(\"Length of original: {}\".format(len(tokenized_sents)))\n",
    "print(\"Length after stopword removal: {} \\n\".format(len(sw_removed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Oft ist es auch nötig, sogenannte N-Gramme zu speichern. Bei diesen handelt es sich einfach nur um Sequenzen von aufeinanderfolgenden\n",
    " Wörtern bzw. Charactern im Corpus, wobei N die konkrete Anzahl davon bestimmt. Man unterscheidet zwischen Wort-N-Grammen und Character-N-Grammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Bigrams:\n",
      " ['Hallo das', 'das ist', 'ist ein', 'ein Test'] \n",
      "\n",
      "Character Bigrams:\n",
      " ['Ha', 'al', 'll', 'lo', 'o ', ' d', 'da', 'as', 's ', ' i', 'is', 'st', 't ', ' e', 'ei', 'in', 'n ', ' T', 'Te', 'es', 'st'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def token_n_grams(text, n=2):\n",
    "    \"\"\" Creates word-n-grams from a given text\n",
    "        Params:\n",
    "            n (int) : The concrete n for the n-grams, so 2 creates Bigrams, 3 creates Trigrams, etc. Default:2\n",
    "            text (list of strings) : The list of tokens thats used to extract n-grams\n",
    "            \n",
    "        Returns:\n",
    "            token_n_grams (list of strings): List of word-n-grams \n",
    "    \"\"\"\n",
    "    token_n_grams = [\" \".join(text[idx:idx+n]) for idx in range(0, len(text)-n+1)]\n",
    "    return token_n_grams\n",
    "\n",
    "def character_n_grams(text, n=2):\n",
    "    \"\"\" Creates character-n-grams from a given text\n",
    "        Params:\n",
    "            n (int) : The concrete n for the n-grams, so 2 creates Bigrams, 3 creates Trigrams, etc. Default:2\n",
    "            text (string) : The text thats used to extract n-grams\n",
    "            \n",
    "        Returns:\n",
    "            char_n_grams (list of strings): List of character-n-grams \n",
    "    \"\"\"\n",
    "    char_n_grams = [text[idx:idx+2] for idx in range(0, len(text)-n+1)]\n",
    "    return char_n_grams\n",
    "\n",
    "sentence = [\"Hallo\", \"das\", \"ist\", \"ein\", \"Test\"]\n",
    "\n",
    "token_bigrams = token_n_grams(sentence)\n",
    "char_bigrams = character_n_grams(\" \".join(sentence))\n",
    "\n",
    "print(\"Token Bigrams:\\n {} \\n\".format(token_bigrams))\n",
    "print(\"Character Bigrams:\\n {} \\n\".format(char_bigrams))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Language Models\n",
    "<br>\n",
    "\n",
    "Language Models können verschiedene NLP Probleme wie Spell Checking, Machine Translation oder auch Speech Recognition lösen. Die Grundidee ist im Endeffekt, für einen bestimmten Satz oder für bestimmte Wörter Wahrscheinlichkeiten auszurechnen.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Spell Checking\n",
    "Beim Spell Checking möchte man z.B. für einen Satz \"The office is about fifteen minuets from my house\" erreichen, dass **P(\"about fifteen minutes from\") > P(\"about fifteen minuets from\")** gilt.\n",
    "\n",
    "#### Machine Translation\n",
    "Bei der Machine Translation möchte man mit den Wahrscheinlichkeiten gute von schlechten Übersetzungen unterscheiden, z.B. \n",
    "**P(\"High winds tonite\") > P(\"Large winds tonite\")**, da erstere eine bessere Übersetzung darstellt.\n",
    "\n",
    "<br>\n",
    "\n",
    "Was wir im Endeffekt also möchten ist ein Model, dass für einen String W = w1....wn die Wahrscheinlichkeit\n",
    "<br>\n",
    "**<p style=\"text-align: center;\">P(W) = P(w1,w2,....,wn)</p>** oder \n",
    "\n",
    "<br>\n",
    "\n",
    "**<p style=\"text-align: center;\">P(wn | w1,w2,...wn-1)</p>**\n",
    "<br>\n",
    "berechnen kann.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Wie werden die Wahrscheinlichkeiten berechnet?\n",
    "Die Wahrscheinlichkeiten werden folgendermaßen berechnet:\n",
    "\n",
    "<br>\n",
    "\n",
    "![Markov Assumption](img/markov-assumption.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Dabei geht es darum, nicht für jedes Wort den gesamten Kontext zu betrachten (zu kompliziert, zu viele Möglichkeiten), sondern man betrachtet für jedes Wort nur k Kontextwörter.\n",
    "<br>\n",
    "Für **k=0** nennt man solche Models **Unigram Models**, diese betrachten keinerlei Kontext, d.h. die Wahrscheinlichkeit eines Satzes ist einfach das Produkt der Wahrscheinlichkeiten aller Einzelwörter. Für **k=1** spricht man von **Bigram Models**, die das jeweils hintere Wort betrachten etc. \n",
    "\n",
    "<br>\n",
    "\n",
    "Die Einzelwahrscheinlichkeiten werden über die **Counts** geschätzt. Für ein Bigram Model ginge das folgendermaßen:\n",
    "\n",
    "<br>\n",
    "\n",
    "![Count Probability](img/count-probability.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Mit c als \"Count-Funktion\" berechnet das also, wie oft **wi-1** und **wi** gemeinsam vorkommen geteilt durch die Anzahl der Vorkommen von **wi-1**. Dies wird nun für ein ausreichend großes Dataset für jedes Wort im Trainingsset berechnet. Das Ergebnis ist ein Language Model, dass dann für einen konkretes Problem verwendet werden kann.\n",
    "<br>\n",
    "In der Praxis werden diese Wahrscheinlichkeiten aus Effizienzgründen aber nicht miteinander multipliziert, sondern man logarithmiert und addiert sie in der Form **P1 x P2 = log(P1) + log(P2)**.\n",
    "\n",
    "<br>\n",
    "    \n",
    "#### Smoothing\n",
    "Da es Daten gibt, die im Testset aber nicht im Trainingsset vorkommen, muss zusätzlich Smoothing eingeführt werden, da sonst Wahrscheinlichkeiten schnell den Wert 0 annehmen und so die Performanz negativ beeinflussen. (Außerdem lässt sich nichts durch 0 teilen, was früher oder später dazu führen würde.)\n",
    "<br>\n",
    "<br>\n",
    "Diese Wörter werden durch ein spezielles Token **< UNK >** kodiert, welches in der Regel die Wörter im Trainingsset ersetzt, die sowieso schon sehr selten vorkommen. Dadurch lassen sich dann N-Gram-Wahrscheinlichkeiten für unbekannte Wörter trainieren, welche man dann auf das Testset anwenden kann.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Add-1-Smoothing\n",
    "Hier addiert man zu den Counts einfach den Wert 1. ( V = Anzahl Wörter, deren count um 1 erhöht wurde )\n",
    "\n",
    "<br>\n",
    "\n",
    "![Add-One-Smoothing](img/add-one.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Diese Art von Smoothing gilt allerdings als schlecht (zumindest für N-Gram-Models), weshalb bessere Alternativen existieren. (z.B. **Extended Interpolated Kneser Ney** oder für extrem große Datasets: **Stupid Backoff**)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Word Vectors\n",
    "<br>\n",
    "\n",
    "In der Computerlinguistik werden Wörter in der Regel als Vektoren dargestellt, um sie maschinell verarbeiten zu können. \n",
    "Dabei können Wörter auf verschiedene Arten als Vektoren dargestellt werden, wobei die konkrete Herangehensweise ziemlich vom Anwendungsfall abhängt. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### One-Hot-Vectors\n",
    "Bei den sogenannten **One-Hot-Vectors** handelt es sich um Vektoren, die nur aus den Werten 0 und 1 bestehen.\n",
    "Diese finden meist Anwendung, wenn man gegeben einem **Vokabular V** bestehend aus n verschiedenen Wörtern und \n",
    "einer **Dokumentensammlung D** für jedes Dokument d speichern möchte, ob das jeweilige Wort in Dokument d vorkommt oder nicht. In diesem Fall spricht von einer **Term-Document-Matrix**.\n",
    "<br>\n",
    "Dabei bestehen die One-Hot-Vectors aus insgesamt n Komponenten (Größe des Vokabulars), wobei der Wert der jeweiligen Vektorkomponente\n",
    "1 ist, wenn das Wort in Dokument d vorkommt und 0 wenn nicht.\n",
    "<br>\n",
    "<br>\n",
    "\"Stapelt\" man die One-Hot-Vectors aller Dokumente, ergibt sich eine Matrix mit N Zeilen und K Spalten, wobei N die Anzahl der Dokumente\n",
    "und K die Vokabulargröße ist. Aus dieser kann man dann direkt ablesen, welche Wörter in welchen Dokumenten vorkommen.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Count-based Vectors\n",
    "Im Gegensatz zu den One-Hot-Vectors können hier die Vektorkomponenten verschiedene Werte >= 0 annehmen. Die Herangehensweise ist die selbe,\n",
    "jedoch wird für jedes Wort nun nicht gespeichert, ob es vorkommt oder nicht, sondern wie oft. Somit lässt sich aus der Designmatrix später ablesen, welche Wörter **wie oft** in welchen Dokumenten vorkommen.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Context-based Word Vectors\n",
    "Für bestimmte NLP Probleme lässt sich das Konzept auch insofern erweitern, dass man für jedes Wort dessen Kontext speichert.\n",
    "Dabei wird eine bestimmte Fensterlänge n festgelegt, und für jedes Wort w(i) im Vokabular wird gespeichert, welches Wort w(j) in der \n",
    "n-Umgebung von Wort w(i) vorkommt. Dies ist wichtig, da sich herausgestellt hat, dass die Bedeutung eines Wortes maßgeblich dadurch\n",
    "bestimmt wird, welche Wörter in seiner Umgebung / in seinem Kontext vorkommen. \n",
    "<br>\n",
    "Dies lässt sich dann in einer Co-Occurence-Matrix darstellen, \n",
    "die die Form V x V besitzt (V = Vokabulargröße).\n",
    "\n",
    "<br>\n",
    "\n",
    "## Co-Occurence-Matrix\n",
    "Eine Co-Occurence_Matrix für das Vokabular V = { all, that, glitters, is, not, gold, well, ends }, der Fenstergröße n = 1 und den zwei Dokumenten:\n",
    "<br>\n",
    "<br>\n",
    "Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "Document 2: \"all is well that ends well\"\n",
    "\n",
    "würde somit folgendermaßen aussehen:\n",
    "<br>\n",
    "\n",
    "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
    "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
    "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
    "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
    "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
    "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
    "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
    "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
    "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
    "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
    "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Dimensionality Reduction / Word Embeddings\n",
    "Da solche Matrizen in der Realität extrem groß werden und viele 0-Werte enthalten, was unter Anderem zu einem hohen Speicherverbrauch führt, verbessert man sie durch **Dimensionality Reduction**, was alle Dimensionen außer den **k wichtigsten** aus den Vektoren eliminiert und so aus den ursprünglichen Vektoren in der Matrix sogenannte **Word embeddings** formt.<br>\n",
    "Diese stellen Information mit deutlich weniger und dafür informativeren Einträgen dar. Konkret lässt sich sagen, dass diese die (semantischen) Beziehungen zwischen den Wörtern trotz der geringeren Anzahl an Einträgen erhalten.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " [[0 2 0 0 0 0 0 0 0 0]\n",
      " [2 0 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 1 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0]]\n",
      "\n",
      "Reduced Matrix:\n",
      " [[ 1.35181032 -0.36285434]\n",
      " [ 0.38189729  2.29882445]\n",
      " [ 1.77174355 -0.30640105]\n",
      " [ 0.21740554  1.06081987]\n",
      " [ 1.8015794  -0.31544332]\n",
      " [ 0.16499552  0.63726979]\n",
      " [ 0.24724139 -0.05811929]\n",
      " [ 0.65538376  0.52269722]\n",
      " [ 0.60297374  0.09914715]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality Reduction with scikit-learn\n",
    "\n",
    "M = np.array([\n",
    "    [0,2,0,0,0,0,0,0,0,0],\n",
    "    [2,0,1,0,1,0,0,0,0,0],\n",
    "    [0,1,0,1,0,0,0,1,1,0],\n",
    "    [0,0,1,0,1,0,0,0,0,0],\n",
    "    [0,1,0,1,0,1,0,1,0,0],\n",
    "    [0,0,0,0,1,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0,0,1],\n",
    "    [0,0,1,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,1,1,0,0]\n",
    "])\n",
    "\n",
    "# Truncated SVD is for _small_ k top components\n",
    "SVD = TruncatedSVD(n_components=2, n_iter=10)\n",
    "M_reduced = SVD.fit_transform(M)\n",
    "\n",
    "print(\"Original Matrix:\\n {}\\n\".format(M))\n",
    "print(\"Reduced Matrix:\\n {}\\n\".format(M_reduced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wortähnlichkeit\n",
    "Mit dem bisherigen Wissen lassen sich nun Ähnlichkeiten zwischen Wörtern berechnen. Dabei geht man oft korpusbasiert vor, d.h. man lädt einen geeigneten Korpus, stellt eine Co-Occurence-Matrix mit geeigneter Kontextgröße auf und erstellt Word embeddings durch SVD.<br>\n",
    "Das allein liefert aber noch keine besonders guten Ergebnisse, u.a. weil besonders häufige und daher nicht informative Wörter die Ähnlichkeitsberechnung zu massiv beeinflussen.<br><br>\n",
    "Es zeigt sich, dass wahrscheinlichkeitsbasierte Modelle mit Gewichtung nach Informativität (Word2Vec, GloVe) gute Performanz erzielen.\n",
    "\n",
    "<br>\n",
    "\n",
    "Eine mögliche Gewichtung ist die **Positive Pointwise Mutual Information (PPMI)**, welche misst, wie sehr das Zusammenauftreten von zwei Wörtern von der durch die Einzelwahrscheinlichkeiten der Wörter zu berechnenden, erwarteten Häufigkeit abweicht. Davon werden nur die positiven Korrelationen einbezogen, da diese am ausgeprägtesten sind.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Pointwise Mutual Information](img/PMI.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Für die PPMI ergibt sich also die Formel max(0, PMI(w1,w2).<br>\n",
    "Die Wahrscheinlichkeiten werden wieder anhand der relativen Häufigkeiten geschätzt:\n",
    "\n",
    "<br>\n",
    "\n",
    "![Probabilities for PMI](img/probabilities-pmi.jpg)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "    # Load GloVe vectors (Note: this might take some minutes to load)\n",
    "    glove_vecs = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(glove_vecs.vocab.keys()))\n",
    "    \n",
    "    print(glove_vecs.most_similar(\"oil\"))\n",
    "    # >>> 'petroleum', 'crude', 'gas', ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Spelling Correction\n",
    "Man unterscheidet zwischen zwei Fehlerarten, die auftreten können:\n",
    "<br>\n",
    "- Non word errors: Fehler, die als Resultat kein richtiges Wort erzeugen.<br>\n",
    "**Beispiel:** umbrella => ubrella\n",
    "<br>\n",
    "<br>\n",
    "- Real word errors: Fehler, die als Resultat trotzdem ein richtiges Wort erzeugen.<br>\n",
    "**Beispiel:** Three => There\n",
    "\n",
    "<br>\n",
    "\n",
    "Erstere sind leicht erkennbar: Jedes Token, was nicht im Dictionary / Vokabular vorkommt, ist ein Error. Dies erfordert ein möglichst großes Vokabular, um möglichst viele Spelling Errors erkennen zu können.\n",
    "<br>\n",
    "Um diese zu beheben, geht man wie folgt vor:\n",
    "1. Stelle Kandidaten auf, also Wörter die möglichst ähnlich dem \"Error-Word\" sind.\n",
    "2. Wähle z. B. anhand **Shortest Weighted Edit Distance** ein Wort aus.\n",
    "<br>\n",
    "\n",
    "Um **Real word errors** zu beheben, geht man ähnlich vor:\n",
    "1. Stelle Kandidaten auf, die eine ähnliche Aussprache besitzen\n",
    "2. Stelle Kandidaten auf, die ähnlich buchstabiert werden.\n",
    "3. Inkludiere das \"Error-Word\" im Candidate set.\n",
    "4. Wähle einen Kandidaten aus anhand z.B. Noisy Channel, Classifier, etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Noisy Channel Model\n",
    "Das Noisy Channel Model berechnet für ein gegebenes, falsch geschriebenes Wort das korrekte Wort mit der höchsten Wahrscheinlichkeit:\n",
    "<br>\n",
    "\n",
    "![Noisy Channel Probability](img/noisy-channel.jpg)\n",
    "<br>\n",
    "**w** = Korrektes Wort (aus dem Candidate set)\n",
    "<br>\n",
    "**x** = falsch geschriebenes Wort\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Candidate generation\n",
    "Die Kandidaten werden wieder anhand der kleinsten Edit-Distanz zum falsch geschriebenen Wort berechnet (meist Edit-Distanz = 1,2). Besser ist es, auch Wörter einzubeziehen, die die selbe Aussprache haben. Generell lässt sich sagen, dass etwa 80% aller Errors Edit-Distanz 1 besitzen.\n",
    "<br>\n",
    "\n",
    "Beispiel:\n",
    "<br>\n",
    "![Candidate generation example](img/candidates-example.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Error probability\n",
    "Was ebenfalls benötigt wird sind die Wahrscheinlichkeiten des Error Models, d.h. für ein falsch geschriebenes Wort x und ein korrektes Wort w\n",
    "\n",
    "<p style=\"text-align: center;\">x = x1,x2,....,xm</p>\n",
    "<p style=\"text-align: center;\">w = w1,w2,....,wn</p>\n",
    "<br>\n",
    "\n",
    "möchten wir **P(x | w)**, also die \"Wahrscheinlichkeit der Korrektur\" ausrechnen.\n",
    "<br>\n",
    "Dafür benötigt man jeweils eine confusion matrix für Deletion, Insertion, Substitution und Transposition (Swapping) mit (im Fall Englisch) 26 Zeilen und 26 Spalten, welche jedem Buchstaben eine Zahl zuordnet, wie oft dieser fälschlicherweise durch einen anderen Buchstaben getippt wird.\n",
    "<br>\n",
    "\n",
    "Beispiel:\n",
    "<br>\n",
    "![Confusion matrix für Substitution](img/confusion-spelling-error.jpg)\n",
    "<br>\n",
    "\n",
    "Hat man diese Matrizen erstellt, lässt sich nun **P(x | w)** berechnen:\n",
    "<br>\n",
    "![Error model probabilities](img/error-probabilities.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "letter_to_idx = {letter: idx for idx, letter in enumerate(letters)}\n",
    "\n",
    "array_sub = [\n",
    "    [0,0,7,1,342,0,0,2,118,0,1,0,0,3,76,0,0,1,35,9,9,0,1,0,5,0],\n",
    "    [0,0,9,9,2,2,3,1,0,0,0,5,11,5,0,10,0,0,2,1,0,0,8,0,0,0],\n",
    "    [6,5,0,16,0,9,5,0,0,0,1,0,7,9,1,10,2,5,39,40,1,3,7,1,1,0],\n",
    "    [1,10,13,0,12,0,5,5,0,0,2,3,7,3,0,1,0,43,30,22,0,0,4,0,2,0],\n",
    "    [388,0,3,11,0,2,2,0,89,0,0,3,0,5,93,0,0,14,12,6,15,0,1,0,18,0],\n",
    "    [0,15,0,3,1,0,5,2,0,0,0,3,4,1,0,0,0,6,4,12,0,0,2,0,0,0],\n",
    "    [4,1,11,11,9,2,0,0,0,1,1,3,0,0,2,1,3,5,13,21,0,0,1,0,3,0],\n",
    "    [1,8,0,3,0,0,0,0,0,0,2,0,12,14,2,3,0,3,1,11,0,0,2,0,0,0,],\n",
    "    [103,0,0,0,146,0,1,0,0,0,0,6,0,0,49,0,0,0,2,1,47,0,2,1,15,0],\n",
    "    [0,1,1,9,0,0,1,0,0,0,0,2,1,0,0,0,0,0,5,0,0,0,0,0,0,0],\n",
    "    [1,2,8,4,1,1,2,5,0,0,0,0,5,0,2,0,0,0,6,0,0,0,4,0,0,3],\n",
    "    [2,10,1,4,0,4,5,6,13,0,1,0,0,14,2,5,0,11,10,2,0,0,0,0,0,0],\n",
    "    [1,3,7,8,0,2,0,6,0,0,4,4,0,180,0,6,0,0,9,15,13,3,2,2,3,0],\n",
    "    [2,7,6,5,3,0,1,19,1,0,4,35,78,0,0,7,0,28,5,7,0,0,1,2,0,2],\n",
    "    [91,1,1,3,116,0,0,0,25,0,2,0,0,0,0,14,0,2,4,14,39,0,0,0,18,0],\n",
    "    [0,11,1,2,0,6,5,0,2,9,0,2,7,6,15,0,0,1,3,6,0,4,1,0,0,0],\n",
    "    [0,0,1,0,0,0,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0,14,0,30,12,2,2,8,2,0,5,8,4,20,1,14,0,0,12,22,4,0,0,1,0,0],\n",
    "    [11,8,27,33,35,4,0,1,0,1,0,27,0,6,1,7,0,14,0,15,0,0,5,3,20,1],\n",
    "    [3,4,9,42,7,5,19,5,0,1,0,14,9,5,5,6,0,11,37,0,0,2,19,0,7,6],\n",
    "    [20,0,0,0,44,0,0,0,64,0,0,0,0,2,43,0,0,4,0,0,0,0,2,0,8,0],\n",
    "    [0,0,7,0,0,3,0,0,0,0,0,1,0,0,1,0,0,0,8,3,0,0,0,0,0,0],\n",
    "    [2,2,1,0,1,0,0,2,0,0,1,0,0,0,0,7,0,6,3,3,1,0,0,0,0,0], \n",
    "    [0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,0,0,0,0,0],\n",
    "    [0,0,2,0,15,0,1,7,15,0,0,0,2,0,6,1,0,7,36,8,5,0,0,1,0,0],\n",
    "    [0,0,0,7,0,0,0,0,0,0,0,7,5,0,0,0,0,2,21,3,0,0,0,0,3,0]\n",
    "]\n",
    "\n",
    "matrix_sub = np.array(array_sub)\n",
    "    \n",
    "def get_matrix_value(matrix, letter_a, letter_b):\n",
    "    idx_1 = letter_to_idx[letter_a]\n",
    "    idx_2 = letter_to_idx[letter_b]\n",
    "    return matrix[idx_1, idx_2]\n",
    "\n",
    "# How often is m substituted by n?\n",
    "print(get_matrix_value(matrix_sub, 'm', 'n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
